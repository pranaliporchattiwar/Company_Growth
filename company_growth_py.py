# -*- coding: utf-8 -*-
"""Company_Growth.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B4Eno8OdxRpTTldP1UDMahP2tZoPrNff
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Data Exploration"""

# Load CSV file
df = pd.read_csv('/content/archive.zip')

# View the first 5 rows
print(df.head())

# View the last 5 rows
print(df.tail())

# Summary of data types and non-null values
df.info()

# Basic statistical data
df.describe()

# Check for missing values

# Check total missing values per column
print(df.isnull().sum())

# Check if any value is missing
print(df.isnull().values.any())

# Check for duplicate values

# Total number of duplicated rows
print(df.duplicated().sum())

# Remove duplicates if needed
df = df.drop_duplicates()

# Check Column Names
print(df.columns)

# check unique values
print(df['date'].unique())

print(df['open'].unique())

print(df['high'].unique())

print(df['low'].unique())

print(df['close'].unique())

print(df['volume'].unique())

print(df['profit or not'].unique())

"""Basic visualization"""

# Numerical Features

num_cols = df.select_dtypes(include=np.number).columns

for col in num_cols:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f"Distribution of {col}")
    plt.show()

# Trend plot
plt.figure(figsize=(12,5))
plt.plot(df['date'], df['close'], label="Close Price")
plt.title("Closing Price Trend")
plt.xlabel("Date")
plt.ylabel("Close Price")
plt.legend()
plt.show()

# Volume distribution
plt.figure(figsize=(6,4))
sns.histplot(df['volume'], bins=50, kde=True)
plt.title("Volume Distribution")
plt.show()

# Correlation Analysis

plt.figure(figsize=(10, 6))
# Drop non-numeric columns before calculating correlation
sns.heatmap(df.drop(columns=['date']).corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

"""A scatter plot to see the relationship between the 'open' and 'close' prices."""

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='open', y='close')
plt.title('Open vs Close Price')
plt.show()

""" visualize the 'volume' over time. We'll need to convert the 'date' column to datetime objects first."""

df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y')
plt.figure(figsize=(12, 6))
sns.lineplot(data=df, x='date', y='volume')
plt.title('Volume Over Time')
plt.xlabel('Date')
plt.ylabel('Volume')
plt.show()

"""Finally, let's look at the distribution of 'profit or not' using a bar plot."""

plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='profit or not')
plt.title('Distribution of Profit or Not')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay

# Feature preparation
# ------------------------------
X = df[['open', 'high', 'low', 'close', 'volume']]
y = df['profit or not']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=False  # keep time order
)

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Random Forest
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("\n===== Random Forest Report =====")
print(classification_report(y_test, y_pred_rf))
print("ROC AUC:", roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))

"""LightGBM Model (Profit or Not)"""

# LightGBM Model (Profit or Not)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, RocCurveDisplay

# 1. Load dataset
df = pd.read_csv("/content/archive.zip")
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df = df.sort_values('date')

# 2. Feature Engineering
df['target'] = df['profit or not'].shift(-1)

for lag in [1,2,3,5,7,14]:
    df[f'close_lag_{lag}'] = df['close'].shift(lag)
    df[f'volume_lag_{lag}'] = df['volume'].shift(lag)

df['roll_mean_5'] = df['close'].shift(1).rolling(window=5).mean()
df['roll_std_5'] = df['close'].shift(1).rolling(window=5).std()
df['roll_mean_10'] = df['close'].shift(1).rolling(window=10).mean()
df['roll_std_10'] = df['close'].shift(1).rolling(window=10).std()

df = df.dropna().reset_index(drop=True)

# 3. Features & Target
X = df.drop(columns=['date', 'profit or not', 'target'])
y = df['target']

split_index = int(len(df)*0.8)
X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

# 4. Train LightGBM
lgb_train = lgb.Dataset(X_train, label=y_train)
lgb_test = lgb.Dataset(X_test, label=y_test, reference=lgb_train)

params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'verbose': -1
}

model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test],
                  num_boost_round=500, callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=50)])

# 5. Evaluate
y_pred = model.predict(X_test, num_iteration=model.best_iteration)
y_pred_class = (y_pred > 0.5).astype(int)

print("\n===== LightGBM Report =====")
print(classification_report(y_test, y_pred_class))
print("ROC AUC:", roc_auc_score(y_test, y_pred))

RocCurveDisplay.from_predictions(y_test, y_pred)
plt.show()

"""LSTM Model"""

# LSTM Model (Profit or Not)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, roc_auc_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# 1. Load dataset
df = pd.read_csv("/content/archive.zip")
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df = df.sort_values('date')

# 2. Define target
df['target'] = df['profit or not'].shift(-1)
df = df.dropna().reset_index(drop=True)

# 3. Features (use OHLCV)
features = ['open', 'high', 'low', 'close', 'volume']
X = df[features].values
y = df['target'].values

# Scale features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# 4. Reshape for LSTM [samples, timesteps, features]
def create_sequences(X, y, time_steps=10):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        Xs.append(X[i:(i+time_steps)])
        ys.append(y[i+time_steps])
    return np.array(Xs), np.array(ys)

time_steps = 10
X_seq, y_seq = create_sequences(X_scaled, y, time_steps)

# Train-test split (time-aware)
split_index = int(len(X_seq) * 0.8)
X_train, X_test = X_seq[:split_index], X_seq[split_index:]
y_train, y_test = y_seq[:split_index], y_seq[split_index:]

# 5. Build LSTM model
model = Sequential([
    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),
    Dropout(0.2),
    LSTM(32),
    Dropout(0.2),
    Dense(1, activation="sigmoid")
])

model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

# 6. Train (with epochs)
history = model.fit(
    X_train, y_train,
    epochs=20,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# 7. Evaluate
y_pred = model.predict(X_test).ravel()
y_pred_class = (y_pred > 0.5).astype(int)

print("\n===== LSTM Report =====")
print(classification_report(y_test, y_pred_class))
print("ROC AUC:", roc_auc_score(y_test, y_pred))

# Plot training history
plt.plot(history.history['loss'], label="Train Loss")
plt.plot(history.history['val_loss'], label="Val Loss")
plt.legend()
plt.title("LSTM Training Loss")
plt.show()

"""Feature Engineering Function"""

import pandas as pd
import numpy as np

def add_technical_indicators(df):
    # Ensure sorted by date
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    df = df.sort_values('date').reset_index(drop=True)

    # 1. Daily return
    df['return'] = df['close'].pct_change()

    # 2. Moving Averages
    df['ma5'] = df['close'].rolling(window=5).mean()
    df['ma10'] = df['close'].rolling(window=10).mean()

    # 3. Exponential Moving Averages
    df['ema5'] = df['close'].ewm(span=5, adjust=False).mean()
    df['ema10'] = df['close'].ewm(span=10, adjust=False).mean()

    # 4. Rolling Volatility
    df['volatility_10'] = df['return'].rolling(window=10).std()

    # 5. RSI (14)
    delta = df['close'].diff()
    gain = np.where(delta > 0, delta, 0)
    loss = np.where(delta < 0, -delta, 0)
    avg_gain = pd.Series(gain).rolling(window=14).mean()
    avg_loss = pd.Series(loss).rolling(window=14).mean()
    rs = avg_gain / (avg_loss + 1e-10)
    df['rsi14'] = 100 - (100 / (1 + rs))

    # 6. MACD & Signal
    ema12 = df['close'].ewm(span=12, adjust=False).mean()
    ema26 = df['close'].ewm(span=26, adjust=False).mean()
    df['macd'] = ema12 - ema26
    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()

    # Drop NaN values from rolling calculations
    df = df.dropna().reset_index(drop=True)

    return df

"""LightGBM with Technical Indicators"""

import lightgbm as lgb
from sklearn.metrics import classification_report, roc_auc_score, RocCurveDisplay
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, RocCurveDisplay
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import talib


# Load & preprocess
df = pd.read_csv("/content/archive.zip")
df = add_technical_indicators(df)

# Define target (shift to predict next day)
df['target'] = df['profit or not'].shift(-1)
df = df.dropna().reset_index(drop=True)

# Features & Target
X = df.drop(columns=['date', 'profit or not', 'target'])
y = df['target']

# Time-aware train/test split
split_index = int(len(df)*0.8)
X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

# LightGBM
lgb_train = lgb.Dataset(X_train, label=y_train)
lgb_test = lgb.Dataset(X_test, label=y_test)

params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'verbose': -1
}

model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test],
                  num_boost_round=500, callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=50)])


# Predictions
y_pred = model.predict(X_test, num_iteration=model.best_iteration)
y_pred_class = (y_pred > 0.5).astype(int)

print("\n===== LightGBM Report =====")
print(classification_report(y_test, y_pred_class))
print("ROC AUC:", roc_auc_score(y_test, y_pred))

# ROC Curve
RocCurveDisplay.from_predictions(y_test, y_pred)
plt.show()

"""LightGBM with Technical Indicators"""

from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.metrics import classification_report, roc_auc_score

# Load & preprocess
df = pd.read_csv("/content/archive.zip")
df = add_technical_indicators(df)

# Define target
df['target'] = df['profit or not'].shift(-1)
df = df.dropna().reset_index(drop=True)

# Features (OHLCV + Indicators)
features = [c for c in df.columns if c not in ['date','profit or not','target']]
X = df[features].values
y = df['target'].values

# Scale features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Convert to sequences
def create_sequences(X, y, time_steps=20):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        Xs.append(X[i:(i+time_steps)])
        ys.append(y[i+time_steps])
    return np.array(Xs), np.array(ys)

time_steps = 20
X_seq, y_seq = create_sequences(X_scaled, y, time_steps)

# Train-test split
split_index = int(len(X_seq) * 0.8)
X_train, X_test = X_seq[:split_index], X_seq[split_index:]
y_train, y_test = y_seq[:split_index], y_seq[split_index:]

# LSTM Model
model = Sequential([
    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),
    Dropout(0.2),
    LSTM(32),
    Dropout(0.2),
    Dense(1, activation="sigmoid")
])

model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

# Train
history = model.fit(X_train, y_train, epochs=30, batch_size=32,
                    validation_split=0.2, verbose=1)

# Evaluate
y_pred = model.predict(X_test).ravel()
y_pred_class = (y_pred > 0.5).astype(int)

print("\n===== LSTM Report =====")
print(classification_report(y_test, y_pred_class))
print("ROC AUC:", roc_auc_score(y_test, y_pred))

# Plot Training Loss
plt.plot(history.history['loss'], label="Train Loss")
plt.plot(history.history['val_loss'], label="Val Loss")
plt.legend()
plt.title("LSTM Training Loss")
plt.show()

import pandas as pd
import numpy as np

# Load your dataset
df = pd.read_csv("/content/archive.zip")

# Make sure 'date' is sorted
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df = df.sort_values('date').reset_index(drop=True)

# 1. Calculate daily returns
df['return'] = df['close'].pct_change()

# 2. Create binary target (1 if next day's return > 0.5%)
threshold = 0.005  # 0.5%
df['target'] = (df['return'].shift(-1) > threshold).astype(int)

# 3. Drop missing values
df = df.dropna().reset_index(drop=True)

print(df[['date','close','return','target']].head(15))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler

# ==============================
# 1. Load & Prepare Data
# ==============================
df = pd.read_csv("/content/archive.zip")
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df = df.sort_values('date').reset_index(drop=True)

# Daily return
df['return'] = df['close'].pct_change()

# Binary target: 1 if tomorrow’s return > 0.5%
threshold = 0.005
df['target'] = (df['return'].shift(-1) > threshold).astype(int)

# Drop NaN rows
df = df.dropna().reset_index(drop=True)

# Features: you can expand with your technical indicators here
features = [col for col in df.columns if col not in ['date','target']]
X = df[features]
y = df['target']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=False
)

# ==============================
# 2. LightGBM Model
# ==============================
lgb_train = lgb.Dataset(X_train, y_train)
lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)

params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'learning_rate': 0.05,
    'num_leaves': 31,
    'verbose': -1
}

lgb_model = lgb.train(
    params,
    lgb_train,
    valid_sets=[lgb_train, lgb_test],
    num_boost_round=200,
    callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=50)]
)

y_pred_lgb = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)
roc_lgb = roc_auc_score(y_test, y_pred_lgb)
print(f"LightGBM ROC AUC: {roc_lgb:.4f}")

# ==============================
# 3. LSTM Model
# ==============================
# Scale features for LSTM
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Reshape for sequences
time_steps = 20
def create_sequences(X, y, time_steps=20):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        Xs.append(X[i:i+time_steps])
        ys.append(y[i+time_steps])
    return np.array(Xs), np.array(ys)

X_seq, y_seq = create_sequences(X_scaled, y.values, time_steps)

# Train/test split (aligned with original)
split_index = int(len(X_seq) * 0.8)
X_train_seq, X_test_seq = X_seq[:split_index], X_seq[split_index:]
y_train_seq, y_test_seq = y_seq[:split_index], y_seq[split_index:]

# LSTM model
lstm_model = Sequential()
lstm_model.add(LSTM(64, input_shape=(time_steps, X_seq.shape[2]), return_sequences=False))
lstm_model.add(Dropout(0.2))
lstm_model.add(Dense(1, activation="sigmoid"))

lstm_model.compile(loss="binary_crossentropy", optimizer=Adam(0.001), metrics=["AUC"])

history = lstm_model.fit(
    X_train_seq, y_train_seq,
    epochs=30,
    batch_size=32,
    validation_split=0.1,
    verbose=1
)

# Predictions
y_pred_lstm = lstm_model.predict(X_test_seq).ravel()
roc_lstm = roc_auc_score(y_test_seq, y_pred_lstm)
print(f"LSTM ROC AUC:    {roc_lstm:.4f}")

# ==============================
# 4. Compare Results
# ==============================
print("\nModel Comparison:")
print(f"LightGBM ROC AUC: {roc_lgb:.4f}")
print(f"LSTM ROC AUC:    {roc_lstm:.4f}")

from sklearn.metrics import roc_curve, auc

# ==============================
# 5. Confusion Matrices
# ==============================
fig, axes = plt.subplots(1, 2, figsize=(12,5))

# LightGBM confusion matrix
y_pred_lgb_labels = (y_pred_lgb > 0.5).astype(int)
cm_lgb = confusion_matrix(y_test, y_pred_lgb_labels)
sns.heatmap(cm_lgb, annot=True, fmt="d", cmap="Blues", ax=axes[0])
axes[0].set_title("Confusion Matrix - LightGBM")
axes[0].set_xlabel("Predicted")
axes[0].set_ylabel("Actual")

# LSTM confusion matrix
y_pred_lstm_labels = (y_pred_lstm > 0.5).astype(int)
cm_lstm = confusion_matrix(y_test_seq, y_pred_lstm_labels)
sns.heatmap(cm_lstm, annot=True, fmt="d", cmap="Greens", ax=axes[1])
axes[1].set_title("Confusion Matrix - LSTM")
axes[1].set_xlabel("Predicted")
axes[1].set_ylabel("Actual")

plt.tight_layout()
plt.show()

# ==============================
# 6. ROC Curves
# ==============================
fpr_lgb, tpr_lgb, _ = roc_curve(y_test, y_pred_lgb)
fpr_lstm, tpr_lstm, _ = roc_curve(y_test_seq, y_pred_lstm)

plt.figure(figsize=(7,6))
plt.plot(fpr_lgb, tpr_lgb, label=f"LightGBM (AUC = {roc_lgb:.3f})")
plt.plot(fpr_lstm, tpr_lstm, label=f"LSTM (AUC = {roc_lstm:.3f})")
plt.plot([0,1], [0,1], 'k--')  # baseline
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.show()

"""CatBoost Classifier"""

from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix

# Train/test split (reuse from before)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=False
)

# CatBoost Model
cat_model = CatBoostClassifier(
    iterations=300,
    learning_rate=0.05,
    depth=6,
    eval_metric='AUC',
    random_seed=42,
    verbose=100
)

cat_model.fit(X_train, y_train, eval_set=(X_test, y_test))

# Predictions
y_pred_cat = cat_model.predict_proba(X_test)[:,1]
roc_cat = roc_auc_score(y_test, y_pred_cat)
print(f"CatBoost ROC AUC: {roc_cat:.4f}")

# Classification Report
y_pred_cat_labels = (y_pred_cat > 0.5).astype(int)
print("\nClassification Report (CatBoost):")
print(classification_report(y_test, y_pred_cat_labels))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from catboost import CatBoostClassifier
import matplotlib.pyplot as plt

# --- Load Data ---
df = pd.read_csv('/content/archive.zip')

# --- Feature Engineering ---
df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y')
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day
df.drop(columns=['date'], inplace=True)

# Features & Target
X = df.drop(columns=['profit or not'])
y = df['profit or not']

# --- Stratified Train/Test Split ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# --- CatBoost Model ---
cat_model = CatBoostClassifier(
    iterations=1000,
    learning_rate=0.05,
    depth=6,
    eval_metric='AUC',
    random_seed=42,
    l2_leaf_reg=3,
    verbose=100
)

# Train with Early Stopping
cat_model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50)

# --- Predictions ---
y_pred_proba = cat_model.predict_proba(X_test)[:, 1]
y_pred = (y_pred_proba > 0.5).astype(int)

# --- Metrics ---
roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"\nCatBoost ROC AUC: {roc_auc:.4f}")
print("\nClassification Report (CatBoost):")
print(classification_report(y_test, y_pred))

# --- Confusion Matrix ---
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')
plt.title("CatBoost - Confusion Matrix")
plt.show()

# --- Feature Importance ---
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': cat_model.get_feature_importance()
}).sort_values(by='Importance', ascending=False)

print("\nTop Features:")
print(feature_importances)

""" Tune CatBoost"""

from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score
import numpy as np

best_model = None
best_score = 0

for depth in [4, 6, 8]:
    for lr in [0.03, 0.05, 0.1]:
        model = CatBoostClassifier(
            iterations=2000,
            learning_rate=lr,
            depth=depth,
            l2_leaf_reg=3,
            eval_metric='AUC',
            random_seed=42,
            verbose=False
        )
        model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=100)
        preds = model.predict_proba(X_test)[:, 1]
        score = roc_auc_score(y_test, preds)
        print(f"Depth={depth}, LR={lr} → ROC AUC={score:.4f}")
        if score > best_score:
            best_score = score
            best_model = model

print(f"\n Best CatBoost ROC AUC: {best_score:.4f}")

"""Logistic Regression Classification"""

# Logistic Regression Classification
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc
import matplotlib.pyplot as plt

# Initialize Logistic Regression with increased iterations and random state for reproducibility
log_reg = LogisticRegression(max_iter=1000, random_state=42)

# Fit the model
log_reg.fit(X_train_scaled, y_train)

# Predict labels and probabilities
y_pred = log_reg.predict(X_test_scaled)
y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]

# Classification report
print("\n===== Logistic Regression Classification Report =====")
print(classification_report(y_test, y_pred))

# ROC AUC Score
roc_auc = roc_auc_score(y_test, y_prob)
print("ROC AUC Score:", roc_auc)

# Plot ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Train Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)

# Save model and scaler
pickle.dump(log_reg, open('log_reg_model.pkl', 'wb'))
pickle.dump(scaler, open('scaler.pkl', 'wb'))
